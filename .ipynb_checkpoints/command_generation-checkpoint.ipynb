{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a894391e-7524-4b87-8394-3f91d89cefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_prefix= f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=Training\n",
    "#SBATCH --output=slurm_outputs/gemma/training_%j.txt\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task 10\n",
    "#SBATCH --time=72:00:00\n",
    "#SBATCH --mem=1024G\n",
    "###SBATCH --mail-user=...\n",
    "###SBATCH --mail-type=ALL\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --gres=gpu:8\n",
    "#SBATCH --partition=gpu100\n",
    "###SBATCH --nodelist=gpu266\n",
    "###SBATCH --reservation=chlgrp_146\n",
    "###SBATCH --nodelist=gpu271\n",
    "###SBATCH --reservation=chlgrp_156\n",
    "#SBATCH --export=NONE\n",
    "#SBATCH --account=chlgrp\n",
    "#unset SLURM_EXPORT_ENV\n",
    "module load python/3.12.8\n",
    "module load cuda/12.4\n",
    "module load tmux\n",
    "source ~/.bashrc\n",
    "source /nfs/scistore23/chlgrp/ezverev/envs/noexec_emb/bin/activate\n",
    "export TRANSFORMERS_CACHE='./transformer_cache'\n",
    "export WANDB_MODE=disabled\n",
    "\"\"\"\n",
    "slurm_prefix_evals= f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=TrainingTinyLlama\n",
    "#SBATCH --output=slurm_outputs/training_%j.txt\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task 10\n",
    "#SBATCH --time=72:00:00\n",
    "#SBATCH --mem=196G\n",
    "###SBATCH --mail-user=...\n",
    "###SBATCH --mail-type=ALL\n",
    "#SBATCH --no-requeue\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --partition=gpu100\n",
    "###SBATCH --nodelist=gpu266\n",
    "###SBATCH --reservation=chlgrp_146\n",
    "###SBATCH --nodelist=gpu271\n",
    "###SBATCH --reservation=chlgrp_156\n",
    "#SBATCH --account=chlgrp\n",
    "\n",
    "#SBATCH --export=NONE\n",
    "#unset SLURM_EXPORT_ENV\n",
    "module load python/3.12.8\n",
    "module load cuda/12.4\n",
    "module load tmux\n",
    "source ~/.bashrc\n",
    "source /nfs/scistore23/chlgrp/ezverev/envs/noexec_emb/bin/activate\n",
    "source ./side-env/bin/activate\n",
    "module load python/3.12.8\n",
    "\n",
    "export HF_TOKEN=\"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\"\n",
    "\n",
    "export HF_HOME='./transformer_cache'\n",
    "export WANDB_MODE=disabled\n",
    "export TORCH_CUDA_ARCH_LIST=\"9.0\"\n",
    "\"\"\"\n",
    "# get_slurm_prefix(\"gpu266\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47f433f-a2a4-4c07-a7c0-0db18fccce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 commands have been written\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Base command (static part)\n",
    "base_command = \"srun --export=ALL deepspeed --num_gpus=8 --master_port=29509 fine-tune.py\"\n",
    "\n",
    "# Dictionary of parameters and their possible values.\n",
    "# The key \"batch_and_accum\" holds the combined parameter string.\n",
    "params = {\n",
    "    \"--model_family\": [\"gemma_3_12b\"],\n",
    "    \"--train_version\": [\"SFTv70\"], # SFTv111\n",
    "    \"--emb_type\": [\"forward_rot\",\"single_emb\", \"ise\"],\n",
    "    \"--model_ix\": [\"1\"],\n",
    "    \"--run_number\": [None],\n",
    "    \"--train_type\": [\"full\"],\n",
    "    \"--num_train_epochs\": [\"3\"],\n",
    "    # Treat these two parameters as a single entry:\n",
    "    \"batch_and_accum\": [\"--per_device_train_batch_size 8 --gradient_accumulation_steps 8\",\n",
    "                       \"--per_device_train_batch_size 4 --gradient_accumulation_steps 8\"],\n",
    "    \"--learning_rate\": [\"1e-6\", \"5e-6\", \"1e-5\", \"2e-5\"],\n",
    "    \"--lr_scheduler_type\": [\"cosine\"],\n",
    "    \"--warmup_ratio\": [\"0\",\"0.1\"],\n",
    "    \"--logging_steps\": [\"10\"],\n",
    "    \"--evaluation_strategy\": [\"epoch\"],\n",
    "    \"--save_strategy\": [\"epoch\"],\n",
    "    \"--eval_steps\": [\"1\"],\n",
    "    \"--save_steps\": [\"1\"],\n",
    "    \"--save_total_limit\": [\"1\"],\n",
    "    \"--load_best_model_at_end\": [\"True\"],\n",
    "    \"--prediction_loss_only\": [\"True\"],\n",
    "    \"--bf16\": [\"True\"],\n",
    "    \"--embedding_init\": [\"rot_isoclinic\"],\n",
    "    \"--rotation_alpha\": [\"1.57079633\"],\n",
    "    \"--learned_rotation\": [\"False\"],\n",
    "    \"--add_linear_shift\": [\"False\"],\n",
    "    \"--rotation_direction\": [\"right\"],\n",
    "    \"--gradual_rotation\": [\"False\"]\n",
    "}\n",
    "\n",
    "# Prepare keys and values for the Cartesian product.\n",
    "keys = list(params.keys())\n",
    "values = list(params.values())\n",
    "\n",
    "# Generate commands for all combinations.\n",
    "commands = []\n",
    "command_num=0\n",
    "for combo in itertools.product(*values):\n",
    "    command = base_command\n",
    "    for key, value in zip(keys, combo):\n",
    "        if key == \"batch_and_accum\":\n",
    "            command += \" \" + value\n",
    "        elif key == \"--run_number\":\n",
    "            command += f\" {key} {command_num}\"\n",
    "        else:\n",
    "            command += f\" {key} {value}\"\n",
    "    command_num += 1\n",
    "    commands.append(command + \"\\n\")\n",
    "\n",
    "# Write the commands to a file, with one newline between each command.\n",
    "with open(\"gemma-3-12b_training_1.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix] + commands[:len(commands)//2]))\n",
    "with open(\"gemma-3-12b_training_2.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix] + commands[len(commands)//2:]))\n",
    "\n",
    "print(f\"{len(commands)} commands have been written\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbfa31b3-f3f8-4685-82f8-928aa682f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 commands have been written\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Base command (static part)\n",
    "base_command = \"srun --export=ALL deepspeed --num_gpus=8 --master_port=29509 fine-tune.py\"\n",
    "\n",
    "# Dictionary of parameters and their possible values.\n",
    "# The key \"batch_and_accum\" holds the combined parameter string.\n",
    "params = {\n",
    "    \"--model_family\": [\"llama_2_13b\"],\n",
    "    \"--train_version\": [\"SFTv110\"], # SFTv111\n",
    "    \"--emb_type\": [\"forward_rot\",\"single_emb\", \"ise\"],\n",
    "    \"--model_ix\": [\"1\"],\n",
    "    \"--run_number\": [None],\n",
    "    \"--train_type\": [\"full\"],\n",
    "    \"--num_train_epochs\": [\"3\"],\n",
    "    # Treat these two parameters as a single entry:\n",
    "    \"batch_and_accum\": [\"--per_device_train_batch_size 2 --gradient_accumulation_steps 4\",\n",
    "                       \"--per_device_train_batch_size 2 --gradient_accumulation_steps 8\"],\n",
    "    \"--learning_rate\": [\"1e-6\", \"5e-6\", \"1e-5\", \"2e-5\"],\n",
    "    \"--lr_scheduler_type\": [\"cosine\"],\n",
    "    \"--warmup_ratio\": [\"0\",\"0.1\"],\n",
    "    \"--logging_steps\": [\"10\"],\n",
    "    \"--evaluation_strategy\": [\"epoch\"],\n",
    "    \"--save_strategy\": [\"epoch\"],\n",
    "    \"--eval_steps\": [\"1\"],\n",
    "    \"--save_steps\": [\"1\"],\n",
    "    \"--save_total_limit\": [\"1\"],\n",
    "    \"--load_best_model_at_end\": [\"True\"],\n",
    "    \"--prediction_loss_only\": [\"True\"],\n",
    "    \"--bf16\": [\"True\"],\n",
    "    \"--embedding_init\": [\"rot_isoclinic\"],\n",
    "    \"--rotation_alpha\": [\"1.57079633\"],\n",
    "    \"--learned_rotation\": [\"False\"],\n",
    "    \"--add_linear_shift\": [\"False\"],\n",
    "    \"--rotation_direction\": [\"right\"],\n",
    "    \"--gradual_rotation\": [\"False\"]\n",
    "}\n",
    "\n",
    "# Prepare keys and values for the Cartesian product.\n",
    "keys = list(params.keys())\n",
    "values = list(params.values())\n",
    "\n",
    "# Generate commands for all combinations.\n",
    "commands = []\n",
    "command_num=0\n",
    "for combo in itertools.product(*values):\n",
    "    command = base_command\n",
    "    for key, value in zip(keys, combo):\n",
    "        if key == \"batch_and_accum\":\n",
    "            command += \" \" + value\n",
    "        elif key == \"--run_number\":\n",
    "            command += f\" {key} {command_num}\"\n",
    "        else:\n",
    "            command += f\" {key} {value}\"\n",
    "    command_num += 1\n",
    "    commands.append(command + \"\\n\")\n",
    "\n",
    "# Write the commands to a file, with one newline between each command.\n",
    "with open(\"llama_2_13b_training_1.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([get_slurm_prefix(\"gpu266\")] + commands[:len(commands)//2]))\n",
    "with open(\"llama_2_13b_training_2.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([get_slurm_prefix(\"gpu271\")] + commands[len(commands)//2:]))\n",
    "\n",
    "print(f\"{len(commands)} commands have been written\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a000d3-5e01-4b59-b283-309594a35482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 commands have been written\n"
     ]
    }
   ],
   "source": [
    "def generate_commands(mapping, model_name, sft):\n",
    "    \"\"\"\n",
    "    mapping: dict of form {\n",
    "        embedding_type_1: (model_type_1, run_number_1),\n",
    "        embedding_type_2: (model_type_2, run_number_2),\n",
    "        ...\n",
    "    }\n",
    "    model_name: str (e.g. \"llama_3.1_8b\")\n",
    "    sft: str (e.g. \"SFTv110\")\n",
    "\n",
    "    Returns a list of commands (strings).\n",
    "    \"\"\"\n",
    "    commands = []\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) get_model_outputs.py\n",
    "    #    Example:\n",
    "    #    srun --export=ALL torchrun --nproc_per_node=1 --master_port=29700 \\\n",
    "    #         get_model_outputs.py <embedding_type> <model_name> 1 <sft> <actual_model_type> <run_number>\n",
    "    # ------------------------------------------------------------------\n",
    "    port = 29700\n",
    "    for i, (embedding_type, (actual_model_type, run_number)) in enumerate(mapping.items()):\n",
    "        port = port + i + 1\n",
    "        cmd = (\n",
    "            f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={port} \"\n",
    "            f\"get_model_outputs.py {embedding_type} {model_name} 1 {sft} {actual_model_type} {run_number}\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) get_alpaca_outputs.py\n",
    "    #    Example:\n",
    "    #    srun --export=ALL torchrun --nproc_per_node=1 --master_port=29600 \\\n",
    "    #         get_alpaca_outputs.py --data-path <data_path> --use-input True \\\n",
    "    #         --model ../models/<model_name>/<actual_model_type>/train_checkpoints/<sft>/from_base_run_<run_number>/last/ \\\n",
    "    #         --embedding-type <embedding_type> --batch-size 32\n",
    "    #\n",
    "    #    Special note in your example:\n",
    "    #    - The first two used data/tatsu-lab/alpaca_farm/eval.json\n",
    "    #    - The last used data/tatsu-lab/alpaca_eval/eval.json\n",
    "    #    Below we switch if embedding_type == \"single_emb\"; adjust as needed.\n",
    "    # ------------------------------------------------------------------\n",
    "    for i, (embedding_type, (actual_model_type, run_number)) in enumerate(mapping.items()):\n",
    "        port = port + i + 1\n",
    "\n",
    "        # Example logic to mimic your snippet:\n",
    "        # Use 'alpaca_farm/eval.json' unless embedding_type == 'single_emb'\n",
    "        if embedding_type == \"single_emb\":\n",
    "            data_path = \"../data/tatsu-lab/alpaca_eval/alpaca_eval_eval.json\"\n",
    "        else:\n",
    "            data_path = \"../data/tatsu-lab/alpaca_farm/alpaca_farm_eval.json\"\n",
    "        use_input = embedding_type !='single_emb'\n",
    "        cmd = (\n",
    "            f\"srun --chdir=evals --export=ALL torchrun --nproc_per_node=1 --master_port={port} \"\n",
    "            f\"get_alpaca_outputs.py --data-path {data_path} {'--use-input True ' if use_input else ''}\"\n",
    "            f\"--model ../models/{model_name}/{actual_model_type}/train_checkpoints/{sft}/from_base_run_{run_number}/last/ \"\n",
    "            f\"--embedding-type {embedding_type} --batch-size 32\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) test_on_struq.py\n",
    "    #    Example:\n",
    "    #    srun --export=ALL torchrun --nproc_per_node=1 --master_port=29901 \\\n",
    "    #         test_on_struq.py --domain all --attack all \\\n",
    "    #         --model ../models/<model_name>/<actual_model_type>/train_checkpoints/<sft>/from_base_run_<run_number>/last/ \\\n",
    "    #         --embedding_type <embedding_type> --batch_size 32\n",
    "    #\n",
    "    #    In your example the ports (29901, 29904, 29905) are not consecutive.\n",
    "    #    You can adjust to consecutive or keep your custom pattern.\n",
    "    # ------------------------------------------------------------------\n",
    "    for i, (embedding_type, (actual_model_type, run_number)) in enumerate(mapping.items()):\n",
    "        # If you want consecutive:\n",
    "        port = port + i + 1\n",
    "        # Or replicate the example’s pattern exactly by embedding_type,\n",
    "        # but that would require a custom mapping.\n",
    "\n",
    "        cmd = (\n",
    "            f\"srun --chdir=struq --export=ALL torchrun --nproc_per_node=1 --master_port={port} \"\n",
    "            f\"test_on_struq.py --domain all --attack all \"\n",
    "            f\"--model ../models/{model_name}/{actual_model_type}/train_checkpoints/{sft}/from_base_run_{run_number}/last/ \"\n",
    "            f\"--embedding_type {embedding_type} --batch_size 32\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "        # ----------------------------------------------------------------\n",
    "        # 4) alpaca_eval commands\n",
    "        #    Example:\n",
    "        #    IS_ALPACA_EVAL_2=False alpaca_eval --model_outputs ./data/tatsu-lab/alpaca_farm/llama_3.1_8b_ise_train_checkpoints_...\n",
    "        #    or for single_emb, use \"./data/tatsu-lab/alpaca_eval/...\"\n",
    "        # ------------------------------------------------------------------\n",
    "    for embedding_type, (actual_model_type, run_number) in mapping.items():\n",
    "        if embedding_type == \"single_emb\":\n",
    "            directory = \"alpaca_eval\"\n",
    "        else:\n",
    "            directory = \"alpaca_farm\"\n",
    "\n",
    "        # Build the JSON filename. Follows the pattern:\n",
    "        # \"./data/tatsu-lab/<directory>/llama_3.1_8b_<actual_model_type>_train_checkpoints_<sft>_from_base_run_<run_number>_last__l-1_s42.json\"\n",
    "        json_path = (\n",
    "            f\"./data/tatsu-lab/{directory}/\"\n",
    "            f\"{model_name}_{actual_model_type}_train_checkpoints_{sft}_from_base_run_{run_number}_last__l-1_s42.json\"\n",
    "        )\n",
    "\n",
    "        cmd = (\n",
    "            f\"IS_ALPACA_EVAL_2=False alpaca_eval --model_outputs {json_path}\"\n",
    "        )\n",
    "        commands.append(cmd)\n",
    "\n",
    "    return commands\n",
    "\n",
    "\n",
    "\n",
    "mapping = {\n",
    "    \"single_emb\": (\"single_emb\", \"27\"),\n",
    "    \"ise\": (\"ise\", \"34\"),\n",
    "    \"forward_rot\": (\"forward_rot\", \"4\"),\n",
    "}\n",
    "model_name = \"gemma-3-4b-pt\"\n",
    "sft = \"SFTv70\"\n",
    "\n",
    "all_commands = generate_commands(mapping, model_name, sft)\n",
    "\n",
    "with open(\"gemma-3-4b_evals.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([slurm_prefix_evals] + all_commands))\n",
    "\n",
    "print(f\"{len(all_commands)} commands have been written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8544c9e-da4d-4b8c-bca3-59537e34f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29700 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 0\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29701 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 1\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29702 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 2\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29703 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 3\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29704 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 4\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29705 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 5\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29706 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 6\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29707 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 7\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29708 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 8\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29709 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 9\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29710 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 10\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29711 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 11\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29712 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 12\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29713 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 13\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29714 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 14\n",
      "srun --export=ALL torchrun --nproc_per_node=1 --master_port=29715 get_model_outputs.py single_emb llama_3.1_8b 1 SFTv111 single_emb 15\n"
     ]
    }
   ],
   "source": [
    "commands = []\n",
    "master_port = 29700\n",
    "emb_type = \"forward_rot\"#\"forward_rot\" \n",
    "sft = \"SFTv111\"\n",
    "for run_number in range(16):\n",
    "    commands.append(\n",
    "        f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={master_port + run_number} get_model_outputs.py {emb_type} llama_3.1_8b 1 {sft} {emb_type} {run_number}\"\n",
    "    )\n",
    "print(\"\\n\".join(commands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "397c71d9-c138-43cd-96ad-073117ca5c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 commands have been written\n"
     ]
    }
   ],
   "source": [
    "commands = []\n",
    "master_port = 29600\n",
    "emb_type = \"forward_rot\"#\"forward_rot\" \n",
    "sft = \"SFTv111\"\n",
    "for i in range(16):\n",
    "    commands.append(\n",
    "        f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={master_port + i} get_alpaca_outputs.py --data-path data/tatsu-lab/alpaca_farm/eval.json --use-input True --model ../models/llama_3.1_8b/{emb_type}/train_checkpoints/{sft}/from_base_run_{i}/last/ --embedding-type forward_rot --batch-size 32\"\n",
    "    )\n",
    "\n",
    "emb_type = \"single_emb\"#\"forward_rot\" \n",
    "sft = \"SFTv111\"\n",
    "for i in range(16):\n",
    "    commands.append(\n",
    "        f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={master_port + i} get_alpaca_outputs.py --data-path data/tatsu-lab/alpaca_farm/eval.json --use-input True --model ../models/llama_3.1_8b/{emb_type}/train_checkpoints/{sft}/from_base_run_{i}/last/ --embedding-type forward_rot --batch-size 32\"\n",
    "    )\n",
    "\n",
    "emb_type = \"forward_rot\"#\"forward_rot\" \n",
    "sft = \"SFTv110\"\n",
    "for i in range(16):\n",
    "    commands.append(\n",
    "        f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={master_port + i} get_alpaca_outputs.py --data-path data/tatsu-lab/alpaca_farm/eval.json --use-input True --model ../models/llama_3.1_8b/{emb_type}/train_checkpoints/{sft}/from_base_run_{i}/last/ --embedding-type forward_rot --batch-size 32\"\n",
    "    )\n",
    "\n",
    "emb_type = \"single_emb\"#\"forward_rot\" \n",
    "sft = \"SFTv110\"\n",
    "for i in range(16):\n",
    "    commands.append(\n",
    "        f\"srun --export=ALL torchrun --nproc_per_node=1 --master_port={master_port + i} get_alpaca_outputs.py --data-path data/tatsu-lab/alpaca_farm/eval.json --use-input True --model ../models/llama_3.1_8b/{emb_type}/train_checkpoints/{sft}/from_base_run_{i}/last/ --embedding-type forward_rot --batch-size 32\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "with open(\"mar31_alpaca.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(commands))\n",
    "print(f\"{len(commands)} commands have been written\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fca4b5f0-6710-4dfb-acbd-852419796197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 commands have been written\n",
      "commands [0: 8] have been written\n",
      "commands [8: 16] have been written\n",
      "commands [16: 24] have been written\n",
      "commands [24: 32] have been written\n",
      "commands [32: 40] have been written\n",
      "commands [40: 48] have been written\n",
      "commands [48: 56] have been written\n",
      "commands [56: 64] have been written\n"
     ]
    }
   ],
   "source": [
    "commands = []\n",
    "\n",
    "for emb_type in [\"forward_rot\", \"pretrained_vanilla\"]:\n",
    "    if emb_type == \"forward_rot\": \n",
    "        alpaca_folder = \"alpaca_farm\"\n",
    "    else:\n",
    "        alpaca_folder = \"alpaca_eval\"\n",
    "    for sft in [\"SFTv110\", \"SFTv111\"]:\n",
    "        for i in range(16):\n",
    "                commands.append(\n",
    "        f\"IS_ALPACA_EVAL_2=False alpaca_eval --model_outputs ./data/tatsu-lab/{alpaca_folder}/llama_3.1_8b_{emb_type}_train_checkpoints_{sft}_from_base_run_{i}_last__l-1_s42.json\"    \n",
    "                )\n",
    "\n",
    "with open(\"get_alpaca_scores_mar31.sh\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([\"#!/usr/bin/env bash\"] + commands))\n",
    "print(f\"{len(commands)} commands have been written\")\n",
    "num_of_nodes = 8\n",
    "for i in range(len(commands) // num_of_nodes): \n",
    "    cur_commands = commands[num_of_nodes * i: num_of_nodes * (i + 1)]\n",
    "    with open(f\"get_alpaca_scores_mar31_{i}.sh\", \"w\") as file:\n",
    "        file.write(\"\\n\".join([\"#!/usr/bin/env bash\"] + cur_commands))\n",
    "    print(f\"commands [{num_of_nodes * i}: {num_of_nodes * (i + 1)}] have been written\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98ce2a-cc90-4002-9343-812bbedba309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
